# BLASSO_SA
MAP estimation of the Bayesian LASSO via Simulated Annealing.

# Sampler
The Simulated Annealing is based on the Gibbs sampler presented in [1] (with marginalized out &#956;).
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{array}{llcl}&space;&&&A&space;=&space;X^TX&space;&plus;&space;D_\tau^{-1}&space;\\&space;&&&D_\tau&space;=&space;diag(\tau_1^2,\dots,\tau_p^2)&space;\\&space;\\&space;\beta&space;&|&space;\quad&space;\tilde{y},&space;\sigma^2,&space;\tau_1^2,&space;\dots,&space;\tau_p^2&space;&\sim&&space;\mathcal{N}(A^{-1}&space;X^T\tilde{y},&space;\sigma^2&space;A^{-1})&space;\\&space;\sigma^2&space;&|\quad\tilde{y},&space;\beta,\tau_1^2,\dots,\tau_p^2&space;&\sim&&space;InvGamma\Big(\frac{1}{2}(n-1&plus;p),&space;\frac{1}{2}&space;\big((\tilde{y}-X\beta)^T&space;(\tilde{y}-X\beta)&plus;&space;\beta^TD_\tau^{-1}\beta\big)\Big)&space;\\&space;1/\tau_j^2&space;&|\quad&space;\tilde{y},&space;\beta,&space;\tau_{-j}^2&space;&\sim&&space;InvGauss\Big(\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}},&space;\lambda'=\lambda^2&space;\Big)&space;\end{array}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{array}{llcl}&space;&&&A&space;=&space;X^TX&space;&plus;&space;D_\tau^{-1}&space;\\&space;&&&D_\tau&space;=&space;diag(\tau_1^2,\dots,\tau_p^2)&space;\\&space;\\&space;\beta&space;&|&space;\quad&space;\tilde{y},&space;\sigma^2,&space;\tau_1^2,&space;\dots,&space;\tau_p^2&space;&\sim&&space;\mathcal{N}(A^{-1}&space;X^T\tilde{y},&space;\sigma^2&space;A^{-1})&space;\\&space;\sigma^2&space;&|\quad\tilde{y},&space;\beta,\tau_1^2,\dots,\tau_p^2&space;&\sim&&space;InvGamma\Big(\frac{1}{2}(n-1&plus;p),&space;\frac{1}{2}&space;\big((\tilde{y}-X\beta)^T&space;(\tilde{y}-X\beta)&plus;&space;\beta^TD_\tau^{-1}\beta\big)\Big)&space;\\&space;1/\tau_j^2&space;&|\quad&space;\tilde{y},&space;\beta,&space;\tau_{-j}^2&space;&\sim&&space;InvGauss\Big(\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}},&space;\lambda'=\lambda^2&space;\Big)&space;\end{array}" title="\begin{array}{llcl} &&&A = X^TX + D_\tau^{-1} \\ &&&D_\tau = diag(\tau_1^2,\dots,\tau_p^2) \\ \\ \beta &| \quad \tilde{y}, \sigma^2, \tau_1^2, \dots, \tau_p^2 &\sim& \mathcal{N}(A^{-1} X^T\tilde{y}, \sigma^2 A^{-1}) \\ \sigma^2 &|\quad\tilde{y}, \beta,\tau_1^2,\dots,\tau_p^2 &\sim& InvGamma\Big(\frac{1}{2}(n-1+p), \frac{1}{2} \big((\tilde{y}-X\beta)^T (\tilde{y}-X\beta)+ \beta^TD_\tau^{-1}\beta\big)\Big) \\ 1/\tau_j^2 &|\quad \tilde{y}, \beta, \tau_{-j}^2 &\sim& InvGauss\Big(\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}}, \lambda'=\lambda^2 \Big) \end{array}" /></a>

Cooling down of the posterior conditionals can be achieved by a parameter shift of the distributions. For the Inverse Gaussian distribution we make use of the fact that we can represent an IG as a <a href="https://en.wikipedia.org/wiki/Generalized_inverse_Gaussian_distribution">Generalized Inverse Gaussian</a>
with p=-0.5.

Let <i>T</i> be the current Temperature. Then we can sample according to:
<a href="https://www.codecogs.com/eqnedit.php?latex=\large&space;\begin{array}{llcl}&space;&&&T&space;\in&space;\mathbb{R}_{>0}&space;\\&space;\\&space;&&&A&space;=&space;X^TX&space;&plus;&space;D_\tau^{-1}&space;\\&space;&&&D_\tau&space;=&space;diag(\tau_1^2,\dots,\tau_p^2)&space;\\&space;\\&space;\beta&space;&|&space;\quad&space;\tilde{y},&space;\sigma^2,&space;\tau_1^2,&space;\dots,&space;\tau_p^2,&space;T&space;&\sim&&space;\mathcal{N}(A^{-1}&space;X^T\tilde{y},&space;\sigma^2&space;T&space;A^{-1})&space;\\&space;\sigma^2&space;&|\quad\tilde{y},&space;\beta,\tau_1^2,\dots,\tau_p^2,&space;T&space;&\sim&&space;InvGamma\Big(\frac{\frac{1}{2}(n-1&plus;p)&plus;1}{T}&space;-1,&space;\frac{\frac{1}{2}&space;\big((\tilde{y}-X\beta)^T&space;(\tilde{y}-X\beta)&plus;&space;\beta^TD_\tau^{-1}\beta\big)}{T}\Big)&space;\\&space;1/\tau_j^2&space;&|\quad&space;\tilde{y},&space;\beta,&space;\tau_{-j}^2,&space;T&space;&\sim&&space;GIG\Big(a'=\frac{\lambda'/\mu'^2}{T},&space;b'=\frac{\lambda'}{T},&space;p'=(-\frac{1.5}{T}&plus;1)\Big)&space;\\&space;&&&\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}}&space;\\&space;&&&\lambda'=\lambda^2&space;\end{array}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\large&space;\begin{array}{llcl}&space;&&&T&space;\in&space;\mathbb{R}_{>0}&space;\\&space;\\&space;&&&A&space;=&space;X^TX&space;&plus;&space;D_\tau^{-1}&space;\\&space;&&&D_\tau&space;=&space;diag(\tau_1^2,\dots,\tau_p^2)&space;\\&space;\\&space;\beta&space;&|&space;\quad&space;\tilde{y},&space;\sigma^2,&space;\tau_1^2,&space;\dots,&space;\tau_p^2,&space;T&space;&\sim&&space;\mathcal{N}(A^{-1}&space;X^T\tilde{y},&space;\sigma^2&space;T&space;A^{-1})&space;\\&space;\sigma^2&space;&|\quad\tilde{y},&space;\beta,\tau_1^2,\dots,\tau_p^2,&space;T&space;&\sim&&space;InvGamma\Big(\frac{\frac{1}{2}(n-1&plus;p)&plus;1}{T}&space;-1,&space;\frac{\frac{1}{2}&space;\big((\tilde{y}-X\beta)^T&space;(\tilde{y}-X\beta)&plus;&space;\beta^TD_\tau^{-1}\beta\big)}{T}\Big)&space;\\&space;1/\tau_j^2&space;&|\quad&space;\tilde{y},&space;\beta,&space;\tau_{-j}^2,&space;T&space;&\sim&&space;GIG\Big(a'=\frac{\lambda'/\mu'^2}{T},&space;b'=\frac{\lambda'}{T},&space;p'=(-\frac{1.5}{T}&plus;1)\Big)&space;\\&space;&&&\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}}&space;\\&space;&&&\lambda'=\lambda^2&space;\end{array}" title="\large \begin{array}{llcl} &&&T \in \mathbb{R}_{>0} \\ \\ &&&A = X^TX + D_\tau^{-1} \\ &&&D_\tau = diag(\tau_1^2,\dots,\tau_p^2) \\ \\ \beta &| \quad \tilde{y}, \sigma^2, \tau_1^2, \dots, \tau_p^2, T &\sim& \mathcal{N}(A^{-1} X^T\tilde{y}, \sigma^2 T A^{-1}) \\ \sigma^2 &|\quad\tilde{y}, \beta,\tau_1^2,\dots,\tau_p^2, T &\sim& InvGamma\Big(\frac{\frac{1}{2}(n-1+p)+1}{T} -1, \frac{\frac{1}{2} \big((\tilde{y}-X\beta)^T (\tilde{y}-X\beta)+ \beta^TD_\tau^{-1}\beta\big)}{T}\Big) \\ 1/\tau_j^2 &|\quad \tilde{y}, \beta, \tau_{-j}^2, T &\sim& GIG\Big(a'=\frac{\lambda'/\mu'^2}{T}, b'=\frac{\lambda'}{T}, p'=(-\frac{1.5}{T}+1)\Big) \\ &&&\mu'=\sqrt{\frac{\lambda^2\sigma^2}{\beta_j^2}} \\ &&&\lambda'=\lambda^2 \end{array}" /></a>


# Sampling from the Normal
We want to avoid having to directly invert A:

<a href="https://www.codecogs.com/eqnedit.php?latex=\large&space;\begin{array}{lcl}&space;T\sigma^2&space;A^{-1}&space;&=&&space;T\sigma^2&space;(L^TL)^{-1}&space;\\&space;&=&&space;T\sigma^2&space;(L^{-T}&space;L^{-1})&space;\\&space;&=&&space;(\sqrt{T}\sigma&space;L^{-T})&space;(L^{-1}\sqrt{T}\sigma)&space;\\&space;\\&space;r&space;=&space;\begin{bmatrix}&space;r_{1}&space;\\&space;\vdots&space;\\&space;r_{p}&space;\end{bmatrix}&space;&\sim&&space;\mathcal{N}(0,&space;I_p)&space;\\&space;\\&space;b&space;=&space;(\sqrt{T}\sigma&space;L^{-T})r&space;&\sim&&space;\mathcal{N}(0,&space;T\sigma^2A^{-1})&space;\\&space;\mu&space;&=&&space;A^{-1}&space;X^T&space;\tilde{y}&space;\\&space;\Leftrightarrow&space;A\mu&=&X^T\tilde{y}&space;\\&space;\Leftrightarrow&space;LL^T\mu&space;&=&&space;X^T&space;\tilde{y}&space;\\&space;b&space;&plus;&space;\mu&space;&\sim&&space;\mathcal{N}(A^{-1}X^T\tilde{y},&space;T\sigma^2A^{-1})&space;\end{array}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\large&space;\begin{array}{lcl}&space;T\sigma^2&space;A^{-1}&space;&=&&space;T\sigma^2&space;(L^TL)^{-1}&space;\\&space;&=&&space;T\sigma^2&space;(L^{-T}&space;L^{-1})&space;\\&space;&=&&space;(\sqrt{T}\sigma&space;L^{-T})&space;(L^{-1}\sqrt{T}\sigma)&space;\\&space;\\&space;r&space;=&space;\begin{bmatrix}&space;r_{1}&space;\\&space;\vdots&space;\\&space;r_{p}&space;\end{bmatrix}&space;&\sim&&space;\mathcal{N}(0,&space;I_p)&space;\\&space;\\&space;b&space;=&space;(\sqrt{T}\sigma&space;L^{-T})r&space;&\sim&&space;\mathcal{N}(0,&space;T\sigma^2A^{-1})&space;\\&space;\mu&space;&=&&space;A^{-1}&space;X^T&space;\tilde{y}&space;\\&space;\Leftrightarrow&space;A\mu&=&X^T\tilde{y}&space;\\&space;\Leftrightarrow&space;LL^T\mu&space;&=&&space;X^T&space;\tilde{y}&space;\\&space;b&space;&plus;&space;\mu&space;&\sim&&space;\mathcal{N}(A^{-1}X^T\tilde{y},&space;T\sigma^2A^{-1})&space;\end{array}" title="\large \begin{array}{lcl} T\sigma^2 A^{-1} &=& T\sigma^2 (L^TL)^{-1} \\ &=& T\sigma^2 (L^{-T} L^{-1}) \\ &=& (\sqrt{T}\sigma L^{-T}) (L^{-1}\sqrt{T}\sigma) \\ \\ r = \begin{bmatrix} r_{1} \\ \vdots \\ r_{p} \end{bmatrix} &\sim& \mathcal{N}(0, I_p) \\ \\ b = (\sqrt{T}\sigma L^{-T})r &\sim& \mathcal{N}(0, T\sigma^2A^{-1}) \\ \mu &=& A^{-1} X^T \tilde{y} \\ \Leftrightarrow A\mu&=&X^T\tilde{y} \\ \Leftrightarrow LL^T\mu &=& X^T \tilde{y} \\ b + \mu &\sim& \mathcal{N}(A^{-1}X^T\tilde{y}, T\sigma^2A^{-1}) \end{array}" /></a>

solve for b by backward subtitution and for &#956; by forward and backward substitution.

# Status
In Progress

# References

[1] <a href="https://www.tandfonline.com/doi/abs/10.1198/016214508000000337">Park, Trevor, and George Casella. "The bayesian lasso." Journal of the American Statistical Association 103.482 (2008): 681-686.</a>
